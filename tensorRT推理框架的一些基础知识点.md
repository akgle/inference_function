TensorRT是一个推理库，而CUDA是一个硬件加速的库，而CUDNN是算子加速的库，而tensorRT推理库在推理的过程会调用硬件加速的库cuda和算子加速的CUDNN库。

1. TensorRT（推理加速库）
主要功能：TensorRT 是一个专门针对深度学习推理优化的库，提供高效的推理加速。
工作方式：TensorRT 通过使用一系列的优化技术（如层融合、精度降低、内存优化等）对训练好的模型进行优化，并在 GPU 上加速推理过程。
如何调用：TensorRT 使用 CUDA 来执行 GPU 上的计算，利用 cuDNN 来加速常见的深度学习操作（如卷积、池化等）。

2. CUDA（硬件加速库）
主要功能：CUDA 是 NVIDIA 提供的并行计算平台，允许软件直接访问和控制 NVIDIA GPU 的计算能力，提供基础的 GPU 加速功能。
工作方式：CUDA 是基础设施，提供了多线程计算能力、内存管理、并行计算等核心功能。它是 TensorRT 和 cuDNN 等库的基础。
如何调用：TensorRT 和 cuDNN 都依赖 CUDA 来执行底层的硬件加速任务。例如，TensorRT 会在 GPU 上执行矩阵计算、内存管理等操作时，直接调用 CUDA API。

3. cuDNN（算子加速库）
主要功能：cuDNN 是专门为深度学习提供优化的数学库，主要加速常见的深度学习操作，特别是卷积、池化、归一化等算子。
工作方式：cuDNN 提供了针对深度学习操作（如卷积、反向传播等）的高效实现，能在 GPU 上执行这些操作时获得更高的性能。
如何调用：TensorRT 在处理深度学习模型时，特别是卷积类操作时，会直接使用 cuDNN 提供的优化算子来提升性能。TensorRT 会根据具体硬件选择合适的 cuDNN 算子，以最大化性能。

关系总结：
TensorRT 是专为推理设计的加速库，依赖 CUDA 提供的硬件加速支持，并利用 cuDNN 提供的优化算子来加速深度学习操作。
CUDA 提供 GPU 硬件的并行计算和内存管理能力，供 TensorRT 和 cuDNN 调用。
cuDNN 提供深度学习算子的优化版本，特别是在卷积、池化等常见操作中，TensorRT 使用这些优化算子来加速推理过程。
通过这样的分工和协作，TensorRT 可以在 GPU 上高效地执行推理任务，而 CUDA 和 cuDNN 则分别提供了底层的硬件加速和深度学习算子的加速支持。







